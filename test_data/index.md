# 边缘侧大语言模型推理加速：从算法到系统

## 前言

随着大语言模型（LLM）和视觉语言模型（VLM）的广泛应用，如何在资源受限的边缘设备上高效部署这些模型成为了关键挑战。本教程面向工业专家和AI科学家，系统性地介绍边缘侧模型推理加速的核心技术，涵盖从底层算法优化到上层系统设计的完整技术栈。

## 目标读者

- 深度学习算法研究员
- AI系统工程师
- 边缘计算架构师
- 对模型压缩与加速有深入需求的技术专家

## 教程特色

- **深入的数学推导**：每个算法都包含完整的数学分析
- **丰富的工程实践**：讨论vLLM、SGLang等主流框架的设计思想
- **全面的技术覆盖**：从量化、剪枝到编译优化的完整技术链
- **实战导向**：针对移动设备、嵌入式系统和笔记本等真实场景

## 章节概览

### 第一部分：理论基础

#### [第1章：边缘推理的挑战与机遇](chapter1.md)
- 1.1 边缘硬件生态：ARM、DSP(Qualcomm Hexagon)、端侧GPU与NPU
- 1.2 模型部署的关键指标
- 1.3 加速技术概览
- 1.4 本教程的技术路线图

#### [第2章：性能分析与Roofline模型](chapter2.md)
- 2.1 Roofline模型基础：计算强度与性能上界
- 2.2 LLM推理的计算特性分析
- 2.3 关键判则：Attention层计算量占比分析
- 2.4 Memory-bound到Compute-bound的转换条件

#### [第3章：小语言模型(SLM)概览](chapter3.md)
- 3.1 主流SLM架构：Phi系列、Gemma系列、Qwen-VL、MiniCPM
- 3.2 SLM的设计权衡与优化
- 3.3 边缘部署的模型选择策略
- 3.4 SLM的典型应用场景

### 第二部分：现代量化技术

#### [第4章：后训练量化（PTQ）](chapter4.md)
- 4.1 GPTQ：最优量化权重量化
- 4.2 AWQ：激活感知权重量化
- 4.3 SmoothQuant：平滑激活异常值
- 4.4 量化粒度与硬件适配

#### [第5章：Hessian引导的量化方法](chapter5.md)
- 5.1 二阶信息在量化中的作用
- 5.2 HAWQ v1：层级混合精度
- 5.3 HAWQ v2/v3：块级别量化
- 5.4 基于Hessian的敏感度分析

#### [第6章：旋转量化与极低比特量化](chapter6.md)
- 6.1 QuaRot：旋转量化的数学原理
- 6.2 INT4/INT2/三值网络
- 6.3 混合精度量化策略
- 6.4 通道分组量化策略

#### [第7章：量化友好的模型设计](chapter7.md)
- 7.1 激活函数选择与量化
- 7.2 归一化层的量化考虑
- 7.3 注意力机制的量化优化设计
- 7.4 量化感知的架构搜索

#### [第8章：量化工具链](chapter8.md)
- 8.1 Bitsandbytes：实用量化库
- 8.2 GGUF格式与llama.cpp
- 8.3 量化感知训练（QAT）实践
- 8.4 量化误差分析与补偿

### 第三部分：模型压缩技术

#### [第9章：模型剪枝](chapter9.md)
- 9.1 结构化剪枝vs非结构化剪枝
- 9.2 渐进式剪枝策略
- 9.3 基于重要性的剪枝准则
- 9.4 剪枝后的微调技术

#### [第10章：稀疏化与参数共享](chapter10.md)
- 10.1 2:4结构化稀疏
- 10.2 N:M稀疏模式设计
- 10.3 共享参数与 model merging
- 10.4 稀疏张量的高效存储

#### [第11章：动态网络架构](chapter11.md)
- 11.1 Slimmable Neural Networks原理
- 11.2 弹性宽度网络设计
- 11.3 早退机制（Early Exit）
- 11.4 Token剪枝与合并（ToMe）

#### [第12章：知识蒸馏](chapter12.md)
- 12.1 传统蒸馏vs特征蒸馏
- 12.2 自蒸馏技术
- 12.3 渐进式蒸馏策略
- 12.4 蒸馏与量化的协同优化

### 第四部分：推理系统优化

#### [第13章：注意力机制优化](chapter13.md)
- 13.1 Flash Attention原理与实现
- 13.2 Multi-Query/Grouped-Query Attention
- 13.3 稀疏注意力模式
- 13.4 线性注意力机制

#### [第14章：KV Cache管理与压缩](chapter14.md)
- 14.1 KV Cache的前缀树管理
- 14.2 动态KV Cache压缩技术
- 14.3 量化KV Cache存储
- 14.4 跨请求Cache复用策略

#### [第15章：解码加速技术](chapter15.md)
- 15.1 投机解码（Speculative Decoding）原理
- 15.2 多Token预测（Multi-token Prediction）
- 15.3 草稿模型设计与选择
- 15.4 并行解码与批量验证

#### [第16章：首Token延迟(TTFT)优化](chapter16.md)
- 16.1 TTFT的关键影响因素
- 16.2 预填充优化技术
- 16.3 混合精度预填充策略
- 16.4 Chunked/Streaming Prefill技术

#### [第17章：内存管理与Offloading](chapter17.md)
- 17.1 CPU-GPU协同内存管理
- 17.2 SSD Offloading技术
- 17.3 Apple Unified Memory优化
- 17.4 NVIDIA Unified Memory架构

#### [第18章：边缘推理框架](chapter18.md)
- 18.1 llama.cpp架构与优化
- 18.2 MediaPipe LLM推理
- 18.3 阿里MNN框架设计
- 18.4 框架选择与对比

### 第五部分：编译器与硬件适配

#### [第19章：深度学习编译器](chapter19.md)
- 19.1 TensorRT工作原理
- 19.2 TVM编译优化技术
- 19.3 ONNX Runtime优化
- 19.4 图优化与算子融合

#### [第20章：硬件特定优化](chapter20.md)
- 20.1 ARM架构优化（Cortex-A/X系列）
- 20.2 Qualcomm Hexagon DSP编程
- 20.3 移动GPU优化（Mali/Adreno）
- 20.4 端侧NPU编程（NNAPI/CoreML）

#### [第21章：跨平台部署实践](chapter21.md)
- 21.1 模型转换最佳实践
- 21.2 性能分析与瓶颈定位
- 21.3 功耗优化策略
- 21.4 边缘-云协同推理

### 第六部分：多模态与实时推理

#### [第22章：视觉编码器优化](chapter22.md)
- 22.1 Vision Transformer加速技术
- 22.2 动态分辨率与自适应计算
- 22.3 视觉特征缓存策略
- 22.4 编码器剪枝与量化

#### [第23章：多模态融合与平衡](chapter23.md)
- 23.1 VLM架构的计算分配
- 23.2 跨模态特征对齐优化
- 23.3 异步编码与流水线设计
- 23.4 动态计算资源调度

#### [第24章：实时语音场景优化](chapter24.md)
- 24.1 流式音频处理架构
- 24.2 语音编码器轻量化
- 24.3 低延迟解码策略
- 24.4 语音-文本-语音闭环优化

### 第七部分：前沿技术与未来展望

#### [第25章：神经架构搜索（NAS）](chapter25.md)
- 25.1 边缘导向的NAS
- 25.2 硬件感知搜索空间
- 25.3 多目标优化策略
- 25.4 自动化压缩流程

#### [第26章：未来技术展望](chapter26.md)
- 26.1 新型量化方法
- 26.2 神经网络与传统算法融合
- 26.3 边缘AI芯片发展趋势
- 26.4 标准化与生态建设

## 附录

### [附录A：数学基础回顾](appendix_a.md)
- 矩阵运算与优化
- 信息论基础
- 凸优化理论

### [附录B：常用评估指标](appendix_b.md)
- 精度评估方法
- 延迟与吞吐量测量
- 能效比计算

### [附录C：工具与资源](appendix_c.md)
- 开源项目列表
- 基准测试套件
- 学习资源推荐

## 使用说明

1. 每章包含详细的数学推导和算法分析
2. 重点讨论软件设计思想，不包含具体代码实现
3. 每章末尾提供6-8道练习题，帮助巩固理解
4. 建议按顺序学习，但各部分也可独立阅读

## 更新说明

本教程将持续更新，跟踪边缘AI领域的最新进展。欢迎反馈和建议。

---
*最后更新：2025年1月*
